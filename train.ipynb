{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./network_dataset\"\n",
    "MAX_DEPTH = 10\n",
    "ATTR_DIM = 3\n",
    "D_MODEL = 64\n",
    "NUM_HEADS = 4\n",
    "ENCODER_LAYERS = 2\n",
    "FF_DIM = 128\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz_files(data_dir):\n",
    "    file_list = sorted(glob.glob(os.path.join(data_dir, \"dataset_part_*.npz\")))\n",
    "    adjacency_all = []\n",
    "    attributes_all = []\n",
    "    latencies_all = []\n",
    "\n",
    "    for f in file_list:\n",
    "        data = np.load(f)\n",
    "        adjacency = data['adjacency']   # shape (N, 10, 10)\n",
    "        attributes = data['attributes'] # shape (N, 10, 3)\n",
    "        latencies = data['latencies']   # shape (N,)\n",
    "\n",
    "        adjacency_all.append(adjacency)\n",
    "        attributes_all.append(attributes)\n",
    "        latencies_all.append(latencies)\n",
    "\n",
    "    adjacency_all = np.concatenate(adjacency_all, axis=0)\n",
    "    attributes_all = np.concatenate(attributes_all, axis=0)\n",
    "    latencies_all = np.concatenate(latencies_all, axis=0)\n",
    "    return adjacency_all, attributes_all, latencies_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(attributes):\n",
    "    node_exists = np.any(attributes != 0, axis=-1)\n",
    "    mask = node_exists\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, max_length, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = self.add_weight(\n",
    "            \"pos_embedding\", shape=(1, max_length, d_model),\n",
    "            initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=dropout)\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(ff_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs, mask=None, training=False):\n",
    "        if mask is not None:\n",
    "            # Expand mask for MultiHeadAttention: (batch, 1, 1, seq_len)\n",
    "            mha_mask = tf.expand_dims(tf.expand_dims(mask, 1), 1)\n",
    "        else:\n",
    "            mha_mask = None\n",
    "\n",
    "        attn_output = self.att(inputs, inputs, attention_mask=mha_mask, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer_model(max_depth=MAX_DEPTH, attr_dim=ATTR_DIM, d_model=D_MODEL, num_heads=NUM_HEADS, ff_dim=FF_DIM, num_layers=ENCODER_LAYERS, dropout=DROPOUT):\n",
    "    inputs = layers.Input(shape=(max_depth, attr_dim))\n",
    "    mask_input = layers.Input(shape=(max_depth,), dtype=tf.bool, name=\"mask_input\")\n",
    "\n",
    "    x = layers.Dense(d_model)(inputs)\n",
    "    x = PositionalEmbedding(max_depth, d_model)(x)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerBlock(d_model, num_heads, ff_dim, dropout)(x, mask=mask_input)\n",
    "\n",
    "    mask_float = tf.cast(mask_input, tf.float32)  \n",
    "    masked_sum = tf.reduce_sum(x * tf.expand_dims(mask_float, -1), axis=1)  \n",
    "    denom = tf.reduce_sum(mask_float, axis=1, keepdims=True) + 1e-9\n",
    "    pooled = masked_sum / denom\n",
    "\n",
    "    outputs = layers.Dense(1)(pooled)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs, mask_input], outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10, 3)]      0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10, 64)       256         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, 10, 64)      640         ['dense[0][0]']                  \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " mask_input (InputLayer)        [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " transformer_block (Transformer  (None, 10, 64)      83200       ['positional_embedding[0][0]',   \n",
      " Block)                                                           'mask_input[0][0]']             \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)           (None, 10)           0           ['mask_input[0][0]']             \n",
      "                                                                                                  \n",
      " transformer_block_1 (Transform  (None, 10, 64)      83200       ['transformer_block[0][0]',      \n",
      " erBlock)                                                         'mask_input[0][0]']             \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 10, 1)        0           ['tf.cast[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 10, 64)       0           ['transformer_block_1[0][0]',    \n",
      "                                                                  'tf.expand_dims[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_1 (TFOpLamb  (None, 1)           0           ['tf.cast[0][0]']                \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum (TFOpLambda  (None, 64)          0           ['tf.math.multiply[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 1)           0           ['tf.math.reduce_sum_1[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.truediv (TFOpLambda)   (None, 64)           0           ['tf.math.reduce_sum[0][0]',     \n",
      "                                                                  'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            65          ['tf.math.truediv[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 167,361\n",
      "Trainable params: 167,361\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 11:09:48.167232: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 3s 16ms/step - loss: 0.2152 - mae: 0.2514 - val_loss: 0.0019 - val_mae: 0.0420\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 2s 20ms/step - loss: 0.0154 - mae: 0.0969 - val_loss: 1.5640e-04 - val_mae: 0.0078\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 3s 21ms/step - loss: 0.0099 - mae: 0.0781 - val_loss: 0.0046 - val_mae: 0.0662\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 3s 22ms/step - loss: 0.0060 - mae: 0.0601 - val_loss: 2.2896e-04 - val_mae: 0.0128\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 3s 22ms/step - loss: 0.0039 - mae: 0.0494 - val_loss: 6.9346e-04 - val_mae: 0.0256\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 3s 23ms/step - loss: 0.0027 - mae: 0.0404 - val_loss: 2.2648e-04 - val_mae: 0.0130\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 3s 21ms/step - loss: 0.0019 - mae: 0.0334 - val_loss: 2.0257e-04 - val_mae: 0.0094\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 3s 25ms/step - loss: 0.0015 - mae: 0.0300 - val_loss: 1.1785e-04 - val_mae: 0.0053\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 3s 25ms/step - loss: 0.0011 - mae: 0.0260 - val_loss: 1.6944e-04 - val_mae: 0.0077\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 3s 21ms/step - loss: 9.2487e-04 - mae: 0.0237 - val_loss: 1.4281e-04 - val_mae: 0.0056\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 3s 21ms/step - loss: 7.9988e-04 - mae: 0.0218 - val_loss: 3.2373e-04 - val_mae: 0.0169\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 3s 24ms/step - loss: 6.0747e-04 - mae: 0.0190 - val_loss: 2.0354e-04 - val_mae: 0.0097\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 3s 26ms/step - loss: 5.1760e-04 - mae: 0.0175 - val_loss: 3.9100e-04 - val_mae: 0.0188\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 3s 24ms/step - loss: 4.5167e-04 - mae: 0.0162 - val_loss: 1.3039e-04 - val_mae: 0.0084\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 3s 24ms/step - loss: 3.8227e-04 - mae: 0.0148 - val_loss: 1.2096e-04 - val_mae: 0.0076\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 3s 26ms/step - loss: 3.4585e-04 - mae: 0.0139 - val_loss: 2.3218e-04 - val_mae: 0.0137\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 3s 25ms/step - loss: 3.0276e-04 - mae: 0.0129 - val_loss: 1.3097e-04 - val_mae: 0.0085\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 3s 24ms/step - loss: 2.8756e-04 - mae: 0.0125 - val_loss: 1.2830e-04 - val_mae: 0.0083\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 3s 26ms/step - loss: 2.8989e-04 - mae: 0.0125 - val_loss: 1.2659e-04 - val_mae: 0.0082\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 3s 23ms/step - loss: 3.1903e-04 - mae: 0.0133 - val_loss: 2.9979e-04 - val_mae: 0.0161\n",
      "Model saved as transformer_latency_model.h5\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.9872e-04 - mae: 0.0163\n",
      "Test Loss: 0.0003, Test MAE: 0.0163\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    adjacency_all, attributes_all, latencies_all = load_npz_files(DATA_DIR)\n",
    "    masks = create_masks(attributes_all)\n",
    "\n",
    "    N = len(latencies_all)\n",
    "    idxs = np.arange(N)\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    val_size = int(VAL_SPLIT * N)\n",
    "    test_size = int(TEST_SPLIT * N)\n",
    "    train_size = N - val_size - test_size\n",
    "\n",
    "    train_idxs = idxs[:train_size]\n",
    "    val_idxs = idxs[train_size:train_size+val_size]\n",
    "    test_idxs = idxs[train_size+val_size:]\n",
    "\n",
    "    X_train = attributes_all[train_idxs]\n",
    "    X_train_mask = masks[train_idxs]\n",
    "    y_train = latencies_all[train_idxs]\n",
    "\n",
    "    X_val = attributes_all[val_idxs]\n",
    "    X_val_mask = masks[val_idxs]\n",
    "    y_val = latencies_all[val_idxs]\n",
    "\n",
    "    X_test = attributes_all[test_idxs]\n",
    "    X_test_mask = masks[test_idxs]\n",
    "    y_test = latencies_all[test_idxs]\n",
    "\n",
    "    model = create_transformer_model()\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(\n",
    "        [X_train, X_train_mask],\n",
    "        y_train,\n",
    "        validation_data=([X_val, X_val_mask], y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    model.save(\"transformer_latency_model.h5\")\n",
    "    print(\"Model saved as transformer_latency_model.h5\")\n",
    "\n",
    "    test_loss, test_mae = model.evaluate([X_test, X_test_mask], y_test, batch_size=BATCH_SIZE)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
