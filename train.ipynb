{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 22:35:43.271484: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734147343.282149 3976704 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734147343.285268 3976704 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-13 22:35:43.297473: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./network_dataset\"\n",
    "MAX_DEPTH = 10\n",
    "ATTR_DIM = 3\n",
    "D_MODEL = 64\n",
    "NUM_HEADS = 4\n",
    "ENCODER_LAYERS = 2\n",
    "FF_DIM = 128\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz_files(data_dir):\n",
    "    file_list = sorted(glob.glob(os.path.join(data_dir, \"dataset_part_*.npz\")))\n",
    "    adjacency_all = []\n",
    "    attributes_all = []\n",
    "    latencies_all = []\n",
    "\n",
    "    for f in file_list:\n",
    "        data = np.load(f)\n",
    "        adjacency = data['adjacency']\n",
    "        attributes = data['attributes']\n",
    "        latencies = data['latencies']\n",
    "\n",
    "        adjacency_all.append(adjacency)\n",
    "        attributes_all.append(attributes)\n",
    "        latencies_all.append(latencies)\n",
    "\n",
    "    adjacency_all = np.concatenate(adjacency_all, axis=0)\n",
    "    attributes_all = np.concatenate(attributes_all, axis=0)\n",
    "    latencies_all = np.concatenate(latencies_all, axis=0)\n",
    "    return adjacency_all, attributes_all, latencies_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(attributes):\n",
    "    node_exists = np.any(attributes != 0, axis=-1)\n",
    "    mask = node_exists\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, max_length, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = self.add_weight(\n",
    "            name=\"pos_embedding\",\n",
    "            shape=(1, max_length, d_model),\n",
    "            initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=dropout)\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(ff_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs, mask=None, training=False):\n",
    "        if mask is not None:\n",
    "            mha_mask = tf.expand_dims(tf.expand_dims(mask, 1), 1)\n",
    "        else:\n",
    "            mha_mask = None\n",
    "\n",
    "        attn_output = self.att(inputs, inputs, attention_mask=mha_mask, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer_model(max_depth=MAX_DEPTH, attr_dim=ATTR_DIM, d_model=D_MODEL, num_heads=NUM_HEADS, ff_dim=FF_DIM, num_layers=ENCODER_LAYERS, dropout=DROPOUT):\n",
    "    inputs = layers.Input(shape=(max_depth, attr_dim))\n",
    "    mask_input = layers.Input(shape=(max_depth,), dtype=tf.bool, name=\"mask_input\")\n",
    "\n",
    "    x = layers.Dense(d_model)(inputs)\n",
    "    x = PositionalEmbedding(max_depth, d_model)(x)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerBlock(d_model, num_heads, ff_dim, dropout)(x, mask=mask_input)\n",
    "\n",
    "    mask_float = layers.Lambda(lambda m: tf.cast(m, tf.float32))(mask_input)\n",
    "    x_masked = layers.Lambda(lambda inputs: inputs[0] * tf.expand_dims(inputs[1], -1))([x, mask_float])\n",
    "    masked_sum = layers.Lambda(lambda xm: tf.reduce_sum(xm, axis=1))(x_masked)\n",
    "    denom = layers.Lambda(lambda mf: tf.reduce_sum(mf, axis=1, keepdims=True) + 1e-9)(mask_float)\n",
    "    pooled = layers.Lambda(lambda inputs: inputs[0] / inputs[1])([masked_sum, denom])\n",
    "\n",
    "    outputs = layers.Dense(1)(pooled)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs, mask_input], outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1734147344.939089 3976704 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mask_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">83,200</span> │ positional_embed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)  │                   │            │ mask_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_1 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">83,200</span> │ transformer_bloc… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)  │                   │            │ mask_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mask_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_bloc… │\n",
       "│                     │                   │            │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ lambda_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ lambda_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m256\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m640\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mask_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m83,200\u001b[0m │ positional_embed… │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)  │                   │            │ mask_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_1 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m83,200\u001b[0m │ transformer_bloc… │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)  │                   │            │ mask_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ mask_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ transformer_bloc… │\n",
       "│                     │                   │            │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_3 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_4 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                     │                   │            │ lambda_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ lambda_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">167,361</span> (653.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m167,361\u001b[0m (653.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">167,361</span> (653.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m167,361\u001b[0m (653.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 21ms/step - loss: 0.0320 - mae: 0.0691 - val_loss: 2.1474e-04 - val_mae: 0.0101\n",
      "Epoch 2/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 2.3475e-04 - mae: 0.0098 - val_loss: 2.3269e-04 - val_mae: 0.0113\n",
      "Epoch 3/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 2.0929e-04 - mae: 0.0087 - val_loss: 2.0760e-04 - val_mae: 0.0065\n",
      "Epoch 4/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 1.9480e-04 - mae: 0.0083 - val_loss: 1.9444e-04 - val_mae: 0.0066\n",
      "Epoch 5/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 1.8297e-04 - mae: 0.0078 - val_loss: 1.8773e-04 - val_mae: 0.0063\n",
      "Epoch 6/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 2.1779e-04 - mae: 0.0075 - val_loss: 2.0241e-04 - val_mae: 0.0077\n",
      "Epoch 7/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 1.7680e-04 - mae: 0.0073 - val_loss: 1.7275e-04 - val_mae: 0.0065\n",
      "Epoch 8/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 1.5937e-04 - mae: 0.0070 - val_loss: 1.5930e-04 - val_mae: 0.0058\n",
      "Epoch 9/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 1.4430e-04 - mae: 0.0066 - val_loss: 1.3941e-04 - val_mae: 0.0049\n",
      "Epoch 10/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 1.1376e-04 - mae: 0.0057 - val_loss: 1.0320e-04 - val_mae: 0.0042\n",
      "Epoch 11/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 20ms/step - loss: 8.4913e-05 - mae: 0.0048 - val_loss: 6.4697e-05 - val_mae: 0.0039\n",
      "Epoch 12/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 7.2736e-05 - mae: 0.0044 - val_loss: 5.3181e-05 - val_mae: 0.0035\n",
      "Epoch 13/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 7.1652e-05 - mae: 0.0044 - val_loss: 9.0464e-05 - val_mae: 0.0051\n",
      "Epoch 14/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 1.0289e-04 - mae: 0.0051 - val_loss: 2.0721e-04 - val_mae: 0.0073\n",
      "Epoch 15/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 1.7881e-04 - mae: 0.0071 - val_loss: 2.0747e-04 - val_mae: 0.0072\n",
      "Epoch 16/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 1.7628e-04 - mae: 0.0071 - val_loss: 2.0756e-04 - val_mae: 0.0070\n",
      "Epoch 17/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 1.8152e-04 - mae: 0.0072 - val_loss: 2.0676e-04 - val_mae: 0.0077\n",
      "Epoch 18/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 1.8502e-04 - mae: 0.0073 - val_loss: 2.0920e-04 - val_mae: 0.0070\n",
      "Epoch 19/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 1.8060e-04 - mae: 0.0073 - val_loss: 2.0822e-04 - val_mae: 0.0071\n",
      "Epoch 20/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 1.7976e-04 - mae: 0.0072 - val_loss: 2.1089e-04 - val_mae: 0.0068\n",
      "Epoch 21/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 1.7661e-04 - mae: 0.0071 - val_loss: 2.1969e-04 - val_mae: 0.0065\n",
      "Epoch 22/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 1.7076e-04 - mae: 0.0070 - val_loss: 2.1061e-04 - val_mae: 0.0068\n",
      "Epoch 23/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 21ms/step - loss: 1.7427e-04 - mae: 0.0071 - val_loss: 2.0790e-04 - val_mae: 0.0072\n",
      "Epoch 24/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 21ms/step - loss: 1.6875e-04 - mae: 0.0069 - val_loss: 1.2836e-04 - val_mae: 0.0046\n",
      "Epoch 25/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 21ms/step - loss: 9.0556e-05 - mae: 0.0048 - val_loss: 8.1038e-05 - val_mae: 0.0044\n",
      "Epoch 26/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 8.3115e-05 - mae: 0.0046 - val_loss: 1.2481e-04 - val_mae: 0.0046\n",
      "Epoch 27/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 21ms/step - loss: 8.9942e-05 - mae: 0.0048 - val_loss: 1.3017e-04 - val_mae: 0.0065\n",
      "Epoch 28/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 21ms/step - loss: 9.3411e-05 - mae: 0.0049 - val_loss: 9.9627e-05 - val_mae: 0.0048\n",
      "Epoch 29/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 9.5737e-05 - mae: 0.0050 - val_loss: 1.2179e-04 - val_mae: 0.0049\n",
      "Epoch 30/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 21ms/step - loss: 9.5673e-05 - mae: 0.0049 - val_loss: 1.0210e-04 - val_mae: 0.0046\n",
      "Epoch 31/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 9.0933e-05 - mae: 0.0048 - val_loss: 1.0161e-04 - val_mae: 0.0045\n",
      "Epoch 32/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 21ms/step - loss: 9.2177e-05 - mae: 0.0049 - val_loss: 1.1030e-04 - val_mae: 0.0057\n",
      "Epoch 33/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 21ms/step - loss: 9.7377e-05 - mae: 0.0050 - val_loss: 1.1435e-04 - val_mae: 0.0045\n",
      "Epoch 34/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 21ms/step - loss: 9.2580e-05 - mae: 0.0049 - val_loss: 9.7551e-05 - val_mae: 0.0052\n",
      "Epoch 35/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 21ms/step - loss: 9.0049e-05 - mae: 0.0049 - val_loss: 1.3990e-04 - val_mae: 0.0048\n",
      "Epoch 36/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 8.9466e-05 - mae: 0.0048 - val_loss: 1.0097e-04 - val_mae: 0.0044\n",
      "Epoch 37/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 21ms/step - loss: 8.5917e-05 - mae: 0.0047 - val_loss: 1.0863e-04 - val_mae: 0.0045\n",
      "Epoch 38/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 21ms/step - loss: 8.6481e-05 - mae: 0.0047 - val_loss: 1.0614e-04 - val_mae: 0.0057\n",
      "Epoch 39/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 22ms/step - loss: 8.8968e-05 - mae: 0.0048 - val_loss: 8.6803e-05 - val_mae: 0.0045\n",
      "Epoch 40/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 9.0454e-05 - mae: 0.0048 - val_loss: 9.3390e-05 - val_mae: 0.0052\n",
      "Epoch 41/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 20ms/step - loss: 8.5619e-05 - mae: 0.0048 - val_loss: 9.2191e-05 - val_mae: 0.0045\n",
      "Epoch 42/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 8.7398e-05 - mae: 0.0047 - val_loss: 8.2255e-05 - val_mae: 0.0049\n",
      "Epoch 43/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 8.3242e-05 - mae: 0.0047 - val_loss: 1.1385e-04 - val_mae: 0.0045\n",
      "Epoch 44/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 8.6107e-05 - mae: 0.0048 - val_loss: 8.6884e-05 - val_mae: 0.0047\n",
      "Epoch 45/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 8.2166e-05 - mae: 0.0047 - val_loss: 8.9244e-05 - val_mae: 0.0044\n",
      "Epoch 46/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 8.4943e-05 - mae: 0.0047 - val_loss: 9.1976e-05 - val_mae: 0.0050\n",
      "Epoch 47/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 8.4375e-05 - mae: 0.0047 - val_loss: 8.5200e-05 - val_mae: 0.0050\n",
      "Epoch 48/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 8.5047e-05 - mae: 0.0047 - val_loss: 7.6092e-05 - val_mae: 0.0042\n",
      "Epoch 49/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 8.3224e-05 - mae: 0.0047 - val_loss: 7.1594e-05 - val_mae: 0.0044\n",
      "Epoch 50/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 7.8376e-05 - mae: 0.0046 - val_loss: 1.2081e-04 - val_mae: 0.0047\n",
      "Epoch 51/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 8.0081e-05 - mae: 0.0046 - val_loss: 8.3439e-05 - val_mae: 0.0047\n",
      "Epoch 52/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 8.0468e-05 - mae: 0.0046 - val_loss: 7.7133e-05 - val_mae: 0.0041\n",
      "Epoch 53/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 7.9550e-05 - mae: 0.0046 - val_loss: 1.2209e-04 - val_mae: 0.0045\n",
      "Epoch 54/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 7.9201e-05 - mae: 0.0045 - val_loss: 7.6114e-05 - val_mae: 0.0041\n",
      "Epoch 55/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 7.8311e-05 - mae: 0.0045 - val_loss: 1.1629e-04 - val_mae: 0.0047\n",
      "Epoch 56/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 7.7813e-05 - mae: 0.0045 - val_loss: 1.0854e-04 - val_mae: 0.0046\n",
      "Epoch 57/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 7.7188e-05 - mae: 0.0045 - val_loss: 6.9080e-05 - val_mae: 0.0040\n",
      "Epoch 58/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 7.1017e-05 - mae: 0.0044 - val_loss: 6.9139e-05 - val_mae: 0.0040\n",
      "Epoch 59/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 7.4974e-05 - mae: 0.0045 - val_loss: 7.5338e-05 - val_mae: 0.0039\n",
      "Epoch 60/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 7.4602e-05 - mae: 0.0045 - val_loss: 1.0129e-04 - val_mae: 0.0056\n",
      "Epoch 61/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 7.1405e-05 - mae: 0.0044 - val_loss: 6.7047e-05 - val_mae: 0.0042\n",
      "Epoch 62/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 7.4981e-05 - mae: 0.0045 - val_loss: 8.7231e-05 - val_mae: 0.0049\n",
      "Epoch 63/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 7.1320e-05 - mae: 0.0044 - val_loss: 7.1551e-05 - val_mae: 0.0042\n",
      "Epoch 64/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 7.1798e-05 - mae: 0.0044 - val_loss: 1.0169e-04 - val_mae: 0.0041\n",
      "Epoch 65/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 6.9424e-05 - mae: 0.0043 - val_loss: 6.1432e-05 - val_mae: 0.0038\n",
      "Epoch 66/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 7.2698e-05 - mae: 0.0044 - val_loss: 6.9278e-05 - val_mae: 0.0040\n",
      "Epoch 67/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 6.9169e-05 - mae: 0.0043 - val_loss: 8.9544e-05 - val_mae: 0.0053\n",
      "Epoch 68/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 7.0062e-05 - mae: 0.0044 - val_loss: 1.7897e-04 - val_mae: 0.0055\n",
      "Epoch 69/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 7.2660e-05 - mae: 0.0044 - val_loss: 1.4522e-04 - val_mae: 0.0067\n",
      "Epoch 70/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 6.9732e-05 - mae: 0.0044 - val_loss: 9.6237e-05 - val_mae: 0.0042\n",
      "Epoch 71/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 6.8047e-05 - mae: 0.0043 - val_loss: 7.5102e-05 - val_mae: 0.0045\n",
      "Epoch 72/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 6.8046e-05 - mae: 0.0043 - val_loss: 7.1952e-05 - val_mae: 0.0040\n",
      "Epoch 73/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 6.6164e-05 - mae: 0.0042 - val_loss: 6.0331e-05 - val_mae: 0.0040\n",
      "Epoch 74/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 6.8646e-05 - mae: 0.0043 - val_loss: 8.3869e-05 - val_mae: 0.0041\n",
      "Epoch 75/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 6.4919e-05 - mae: 0.0042 - val_loss: 7.3456e-05 - val_mae: 0.0040\n",
      "Epoch 76/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 6.6017e-05 - mae: 0.0043 - val_loss: 6.2380e-05 - val_mae: 0.0039\n",
      "Epoch 77/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 6.5113e-05 - mae: 0.0042 - val_loss: 7.6998e-05 - val_mae: 0.0040\n",
      "Epoch 78/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 20ms/step - loss: 6.7694e-05 - mae: 0.0043 - val_loss: 1.0083e-04 - val_mae: 0.0042\n",
      "Epoch 79/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 6.7259e-05 - mae: 0.0043 - val_loss: 6.6633e-05 - val_mae: 0.0038\n",
      "Epoch 80/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 6.2308e-05 - mae: 0.0042 - val_loss: 5.9837e-05 - val_mae: 0.0037\n",
      "Epoch 81/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 6.6461e-05 - mae: 0.0042 - val_loss: 6.6916e-05 - val_mae: 0.0040\n",
      "Epoch 82/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 6.4291e-05 - mae: 0.0042 - val_loss: 8.4049e-05 - val_mae: 0.0041\n",
      "Epoch 83/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 6.5296e-05 - mae: 0.0042 - val_loss: 5.9707e-05 - val_mae: 0.0039\n",
      "Epoch 84/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 6.4921e-05 - mae: 0.0042 - val_loss: 1.1892e-04 - val_mae: 0.0059\n",
      "Epoch 85/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 6.4748e-05 - mae: 0.0042 - val_loss: 7.3185e-05 - val_mae: 0.0040\n",
      "Epoch 86/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 6.2689e-05 - mae: 0.0042 - val_loss: 7.9267e-05 - val_mae: 0.0040\n",
      "Epoch 87/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 6.2787e-05 - mae: 0.0042 - val_loss: 6.2029e-05 - val_mae: 0.0037\n",
      "Epoch 88/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 6.1554e-05 - mae: 0.0042 - val_loss: 6.4951e-05 - val_mae: 0.0041\n",
      "Epoch 89/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 6.3380e-05 - mae: 0.0042 - val_loss: 5.5687e-05 - val_mae: 0.0038\n",
      "Epoch 90/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 6.0999e-05 - mae: 0.0041 - val_loss: 5.9291e-05 - val_mae: 0.0040\n",
      "Epoch 91/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 6.1193e-05 - mae: 0.0041 - val_loss: 1.1206e-04 - val_mae: 0.0043\n",
      "Epoch 92/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 20ms/step - loss: 6.2483e-05 - mae: 0.0042 - val_loss: 8.6320e-05 - val_mae: 0.0039\n",
      "Epoch 93/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 6.1879e-05 - mae: 0.0042 - val_loss: 6.9870e-05 - val_mae: 0.0043\n",
      "Epoch 94/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 5.8578e-05 - mae: 0.0041 - val_loss: 6.5661e-05 - val_mae: 0.0041\n",
      "Epoch 95/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 6.0747e-05 - mae: 0.0041 - val_loss: 1.0011e-04 - val_mae: 0.0046\n",
      "Epoch 96/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 6.2791e-05 - mae: 0.0042 - val_loss: 5.9209e-05 - val_mae: 0.0037\n",
      "Epoch 97/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 6.0943e-05 - mae: 0.0041 - val_loss: 5.9769e-05 - val_mae: 0.0037\n",
      "Epoch 98/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 21ms/step - loss: 6.0954e-05 - mae: 0.0041 - val_loss: 6.2371e-05 - val_mae: 0.0043\n",
      "Epoch 99/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 5.9528e-05 - mae: 0.0041 - val_loss: 5.6369e-05 - val_mae: 0.0040\n",
      "Epoch 100/100\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 21ms/step - loss: 5.9738e-05 - mae: 0.0041 - val_loss: 7.8356e-05 - val_mae: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as transformer_latency_model.h5\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 6.4854e-05 - mae: 0.0038\n",
      "Test Loss: 0.0001, Test MAE: 0.0038\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    adjacency_all, attributes_all, latencies_all = load_npz_files(DATA_DIR)\n",
    "    masks = create_masks(attributes_all)\n",
    "\n",
    "    N = len(latencies_all)\n",
    "    idxs = np.arange(N)\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    val_size = int(VAL_SPLIT * N)\n",
    "    test_size = int(TEST_SPLIT * N)\n",
    "    train_size = N - val_size - test_size\n",
    "\n",
    "    train_idxs = idxs[:train_size]\n",
    "    val_idxs = idxs[train_size:train_size+val_size]\n",
    "    test_idxs = idxs[train_size+val_size:]\n",
    "\n",
    "    X_train = attributes_all[train_idxs]\n",
    "    X_train_mask = masks[train_idxs]\n",
    "    y_train = latencies_all[train_idxs]\n",
    "\n",
    "    X_val = attributes_all[val_idxs]\n",
    "    X_val_mask = masks[val_idxs]\n",
    "    y_val = latencies_all[val_idxs]\n",
    "\n",
    "    X_test = attributes_all[test_idxs]\n",
    "    X_test_mask = masks[test_idxs]\n",
    "    y_test = latencies_all[test_idxs]\n",
    "\n",
    "    model = create_transformer_model()\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(\n",
    "        [X_train, X_train_mask],\n",
    "        y_train,\n",
    "        validation_data=([X_val, X_val_mask], y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    model.save(\"transformer_latency_model.h5\")\n",
    "    print(\"Model saved as transformer_latency_model.h5\")\n",
    "\n",
    "    test_loss, test_mae = model.evaluate([X_test, X_test_mask], y_test, batch_size=BATCH_SIZE)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
